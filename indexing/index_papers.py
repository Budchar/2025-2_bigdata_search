#!/usr/bin/env python3
"""
[ë¬´ë£Œ ë²„ì „] ë…¼ë¬¸ PDFë¥¼ ì²­í‚¹ ë° ë¡œì»¬ ì„ë² ë”©(HuggingFace)í•˜ì—¬ Elasticsearchì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„° + BM25)ì„ ì§€ì›í•˜ëŠ” ìŠ¤í‚¤ë§ˆë¡œ ì¸ë±ì‹±
"""

import argparse
import sys
from pathlib import Path
from typing import List

from elasticsearch import Elasticsearch

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_core.documents import Document
from langchain_elasticsearch import ElasticsearchStore

# ğŸ’¡ ë³€ê²½ì : OpenAIEmbeddings ëŒ€ì‹  HuggingFaceEmbeddings ì‚¬ìš©
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter


class LocalRagIndexer:
    # ì„ë² ë”© ëª¨ë¸ì˜ ì¶œë ¥ ì°¨ì› (jhgan/ko-sbert-multitaskëŠ” 768ì°¨ì›)
    EMBEDDING_DIM = 768

    def __init__(self, es_url: str, index_name: str, device: str = "cpu"):
        """
        ì´ˆê¸°í™” ë° ì„¤ì •

        Args:
            device: 'cpu' ë˜ëŠ” 'cuda' (GPUê°€ ìˆìœ¼ë©´ 'cuda' ê¶Œì¥)
        """
        self.es_url = es_url
        self.index_name = index_name

        print(
            "ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"
        )

        # ğŸ’¡ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ:
        # 1. ì˜ì–´ ë…¼ë¬¸ ìœ„ì£¼ë¼ë©´: "sentence-transformers/all-MiniLM-L6-v2" (ê°€ë³ê³  ë¹ ë¦„)
        # 2. í•œê¸€ ë…¼ë¬¸ ìœ„ì£¼ë¼ë©´: "jhgan/ko-sbert-multitask" (í•œêµ­ì–´ ì„±ëŠ¥ ì¢‹ìŒ)
        # 3. ë‹¤êµ­ì–´(í•œ/ì˜ í˜¼ìš©): "intfloat/multilingual-e5-small"

        model_name = "jhgan/ko-sbert-multitask"  # í•œêµ­ì–´/ì˜ì–´ ë…¼ë¬¸ìš© ì¶”ì²œ ëª¨ë¸

        model_kwargs = {"device": device}
        encode_kwargs = {
            "normalize_embeddings": True
        }  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´ ì •ê·œí™”

        try:
            self.embedding = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs,
            )
            print(f"âœ… ë¡œì»¬ ëª¨ë¸({model_name}) ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit(1)

        # Elasticsearch í´ë¼ì´ì–¸íŠ¸ (ì¸ë±ìŠ¤ ì„¤ì •ìš©)
        self.es_client = Elasticsearch(self.es_url)

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ë§¤í•‘ ì„¤ì •
        self._setup_hybrid_index()

        # Elasticsearch ì—°ê²°
        self.vector_store = ElasticsearchStore(
            es_url=self.es_url,
            index_name=self.index_name,
            embedding=self.embedding,
        )

    def _setup_hybrid_index(self):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„° + BM25)ì„ ìœ„í•œ ì¸ë±ìŠ¤ ë§¤í•‘ ì„¤ì •
        - text í•„ë“œ: BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ìš© (í•œêµ­ì–´/ì˜ì–´ ë¶„ì„ê¸° ì ìš©)
        - vector í•„ë“œ: kNN ë²¡í„° ê²€ìƒ‰ìš©
        """
        # ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„± (ê°œë°œ í™˜ê²½ìš©)
        if self.es_client.indices.exists(index=self.index_name):
            print(f"âš ï¸  ê¸°ì¡´ ì¸ë±ìŠ¤ '{self.index_name}' ë°œê²¬. ì‚­ì œ í›„ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
            self.es_client.indices.delete(index=self.index_name)

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ë§¤í•‘
        index_settings = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "korean_english": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop"],
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    # ë²¡í„° ê²€ìƒ‰ìš© í•„ë“œ (LangChain ElasticsearchStore ê¸°ë³¸ í•„ë“œëª…)
                    "vector": {
                        "type": "dense_vector",
                        "dims": self.EMBEDDING_DIM,
                        "index": True,
                        "similarity": "cosine",  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©
                    },
                    # BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ í•„ë“œ
                    "text": {
                        "type": "text",
                        "analyzer": "korean_english",
                        "fields": {"keyword": {"type": "keyword", "ignore_above": 256}},
                    },
                    # ë©”íƒ€ë°ì´í„° í•„ë“œë“¤
                    "metadata": {
                        "type": "object",
                        "properties": {
                            "source": {"type": "keyword"},
                            "page": {"type": "integer"},
                            "chunk_id": {"type": "keyword"},
                        },
                    },
                }
            },
        }

        # ì¸ë±ìŠ¤ ìƒì„±
        self.es_client.indices.create(index=self.index_name, body=index_settings)
        print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš© ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì™„ë£Œ!")

    def load_documents(self, path: Path, recursive: bool = False) -> List[Document]:
        """PDF íŒŒì¼ ë¡œë“œ"""
        print(f"ğŸ“‚ ë¬¸ì„œ ë¡œë”© ì¤‘... ê²½ë¡œ: {path}")

        if path.is_file():
            loader = PyPDFLoader(str(path))
            docs = loader.load()
        else:
            loader = DirectoryLoader(
                str(path),
                glob="**/*.pdf" if recursive else "*.pdf",
                loader_cls=PyPDFLoader,
                show_progress=True,
            )
            docs = loader.load()

        print(f"âœ… ë¡œë”© ì™„ë£Œ: ì´ {len(docs)} í˜ì´ì§€")
        return docs

    def split_documents(self, docs: List[Document]) -> List[Document]:
        """ë¬¸ì„œ ì²­í‚¹"""
        print("âœ‚ï¸  ë¬¸ì„œ ì²­í‚¹(Splitting) ì§„í–‰ ì¤‘...")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,  # í•œê¸€/ì˜ì–´ í˜¼ìš© ì‹œ 500~700 ì •ë„ê°€ ì ë‹¹
            chunk_overlap=100,
            separators=["\n\n", "\n", ". ", " ", ""],
        )

        splits = text_splitter.split_documents(docs)
        print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(splits)} ê°œì˜ ì²­í¬ ìƒì„±ë¨")
        return splits

    def index_documents(self, splits: List[Document]):
        """ë²¡í„° ì¸ë±ì‹± ìˆ˜í–‰"""
        print(f"ğŸš€ Elasticsearch({self.es_url})ì— ë²¡í„° ì¸ë±ì‹± ì‹œì‘...")
        print("â³ ë¡œì»¬ CPU/GPUë¡œ ë³€í™˜í•˜ë¯€ë¡œ ë¬¸ì„œ ì–‘ì— ë”°ë¼ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        try:
            # ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì¡°ì ˆí•˜ì—¬ ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€ (í•œ ë²ˆì— 32ê°œì”© ì²˜ë¦¬)
            batch_size = 32
            total_splits = len(splits)

            for i in range(0, total_splits, batch_size):
                batch = splits[i : i + batch_size]
                self.vector_store.add_documents(batch)
                print(
                    f"   ... ì§„í–‰ë¥ : {min(i + batch_size, total_splits)} / {total_splits} ì™„ë£Œ"
                )

            print("âœ¨ ëª¨ë“  ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ!")

        except Exception as e:
            print(f"âŒ ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def main():
    parser = argparse.ArgumentParser(description="[ë¬´ë£Œ] RAGìš© PDF ë²¡í„° ì¸ë±ì‹± ë„êµ¬")
    parser.add_argument("path", type=str, help="PDF íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--host", type=str, default="localhost", help="ES í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=9200, help="ES í¬íŠ¸")
    parser.add_argument(
        "--index", type=str, default="papers-rag-local", help="ì¸ë±ìŠ¤ ì´ë¦„"
    )
    parser.add_argument("--recursive", action="store_true", help="í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨")
    # GPU ì‚¬ìš© ì—¬ë¶€ ì˜µì…˜ ì¶”ê°€
    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        choices=["cpu", "cuda"],
        help="ì‹¤í–‰ ë””ë°”ì´ìŠ¤ (cpu/cuda)",
    )

    args = parser.parse_args()

    es_url = f"http://{args.host}:{args.port}"

    # 1. ì´ˆê¸°í™”
    indexer = LocalRagIndexer(es_url, args.index, device=args.device)

    # 2. ë¡œë“œ
    target_path = Path(args.path)
    if not target_path.exists():
        print("âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    raw_docs = indexer.load_documents(target_path, args.recursive)
    if not raw_docs:
        print("âš ï¸ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(0)

    # 3. ì²­í‚¹
    chunks = indexer.split_documents(raw_docs)

    # 4. ì¸ë±ì‹±
    indexer.index_documents(chunks)


if __name__ == "__main__":
    main()
